{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering con Pipeline\\n",
    "**StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\\n",
    "from pyspark.ml.feature import (\\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\\n",
    ")\\n",
    "from pyspark.ml import Pipeline\\n",
    "from pyspark.sql.functions import col, when, isnull, upper\\n",
    "import pandas as pd\\n",
    "\\n",
    "spark = SparkSession.builder \\\\\\n",
    "    .appName(\"SECOP_FeatureEngineering\") \\\\\\n",
    "    .master(\"local[*]\") \\\\\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos limpios del EDA\\n",
    "df = spark.read.parquet(\"/opt/spark-data/processed/secop_eda.parquet\")\\n",
    "print(f\"Registros cargados: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 1: Seleccionar features categóricas y numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"departamento\", \"tipo_de_contrato\", \"estado_contrato\", \"modalidad_de_contratacion\", \"sector\", \"orden\"]\\n",
    "numeric_cols = [\"dias_adicionados_num\"]\\n",
    "target_col = \"valor_del_contrato_num\"\\n",
    "\\n",
    "# Verificar existencia\\n",
    "available_cat = [c for c in categorical_cols if c in df.columns]\\n",
    "available_num = [c for c in numeric_cols if c in df.columns]\\n",
    "print(f\"Categóricas: {available_cat}\")\\n",
    "print(f\"Numéricas: {available_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 2: Estrategia de limpieza (ya aplicada en EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar nulos\\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in available_cat + available_num + [target_col]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 3: VectorAssembler para combinar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in available_cat]\\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_vec\") for col in available_cat]\\n",
    "\\n",
    "feature_cols = available_num + [col+\"_vec\" for col in available_cat]\\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 4: Pipeline completo (orden correcto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stages = indexers + encoders + [assembler]\\n",
    "pipeline = Pipeline(stages=pipeline_stages)\\n",
    "print(f\"Pipeline con {len(pipeline_stages)} stages\")\\n",
    "pipeline_model = pipeline.fit(df)\\n",
    "df_transformed = pipeline_model.transform(df)\\n",
    "df_transformed.select(\"features_raw\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1: Calcular dimensión total de features post-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vec = df_transformed.select(\"features_raw\").first()[0]\\n",
    "dimension_raw = len(sample_vec)\\n",
    "print(f\"Dimensión del vector features_raw: {dimension_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalamiento y PCA (reducción de dimensionalidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\", withStd=True, withMean=True)\\n",
    "scaler_model = scaler.fit(df_transformed)\\n",
    "df_scaled = scaler_model.transform(df_transformed)\\n",
    "\\n",
    "pca = PCA(k=20, inputCol=\"features_scaled\", outputCol=\"features_pca\")\\n",
    "pca_model = pca.fit(df_scaled)\\n",
    "df_final = pca_model.transform(df_scaled)\\n",
    "print(f\"Varianza explicada por los primeros 20 componentes: {pca_model.explainedVariance.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2: Análisis de varianza de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\\n",
    "explained_var = pca_model.explainedVariance\\n",
    "cumulative_var = [sum(explained_var[:i+1]) for i in range(len(explained_var))]\\n",
    "\\n",
    "plt.figure(figsize=(10,5))\\n",
    "plt.plot(range(1,21), cumulative_var, marker='o')\\n",
    "plt.xlabel('Número de componentes')\\n",
    "plt.ylabel('Varianza acumulada explicada')\\n",
    "plt.title('Análisis de Varianza - PCA')\\n",
    "plt.grid()\\n",
    "plt.savefig('/opt/spark-data/processed/pca_variance.png', dpi=150)\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar pipeline y dataset listo para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_path = \"/opt/spark-data/processed/feature_pipeline\"\\n",
    "pipeline_model.save(pipeline_path)\\n",
    "print(f\"Pipeline guardado en: {pipeline_path}\")\\n",
    "\\n",
    "output_path = \"/opt/spark-data/processed/secop_ml_ready.parquet\"\\n",
    "df_final.select(target_col, \"features_pca\").withColumnRenamed(target_col, \"label\").write.mode(\"overwrite\").parquet(output_path)\\n",
    "print(f\"Dataset listo para ML guardado en: {output_path}\")\\n",
    "\\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}