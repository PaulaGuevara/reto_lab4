{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f2b0eb-3706-45ec-9030-d4c5aea27341",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook 09: Optimización de Hiperparámetros\n",
    "\n",
    "**Objetivo**: Comparar Grid Search + CV frente a Train-Validation Split (TVS) para maximizar el AUC del modelo de contratos de alto valor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154413ee-88fd-4495-b7c0-e80776bb74a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/15 05:36:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql.functions import col, when\n",
    "import time\n",
    "import json\n",
    "\n",
    "# %%\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_HyperparameterTuning_Logistica\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11bd51-db45-4e3a-8993-a65b7d3c0fab",
   "metadata": {},
   "source": [
    "\n",
    " ### PASO 0: Preparación de Datos (Binarización)\n",
    " Replicamos la lógica para asegurar que 'label' sea 0 o 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379248a7-3ad0-424a-ae29-68fd566d7dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.parquet(\"/opt/spark-data/processed/secop_ml_ready.parquet\")\n",
    "discretizer = QuantileDiscretizer(numBuckets=4, inputCol=\"label\", outputCol=\"cuartil\")\n",
    "df_final = discretizer.fit(df_raw).transform(df_raw) \\\n",
    "    .withColumn(\"label_bin\", when(col(\"cuartil\") == 3.0, 1.0).otherwise(0.0)) \\\n",
    "    .select(\"features\", col(\"label_bin\").alias(\"label\"))\n",
    "\n",
    "train, test = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# %%\n",
    "# Modelo base y evaluador\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7133c72-3f39-407f-ac6d-1b24f94a5da6",
   "metadata": {},
   "source": [
    "\n",
    " ## RETO 1: Diseñar el Grid de Hiperparámetros\n",
    "\n",
    "**Pregunta de diseño**: Usamos escala logarítmica (0.01, 0.1, 1.0) porque la regularización \n",
    " afecta el modelo en órdenes de magnitud; cambios pequeños lineales suelen ser irrelevantes.\n",
    "\n",
    " **Cálculo**: 3 (reg) x 2 (elastic) x 2 (iter) = **12 combinaciones**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1b2c64-e29d-47a3-afec-30ec10d08920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones totales: 12\n",
      "Total modelos con K=3: 36\n"
     ]
    }
   ],
   "source": [
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Combinaciones totales: {len(grid)}\")\n",
    "print(f\"Total modelos con K=3: {len(grid) * 3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1117b-d0bf-4d24-bf29-928bfc825175",
   "metadata": {},
   "source": [
    "## RETO 2: Grid Search con Cross-Validation\n",
    "\n",
    "**Pregunta**: Usamos K=3 porque el dataset es suficientemente grande para ser representativo \n",
    " y queremos ahorrar tiempo de cómputo en el clúster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2ace43-0f7c-4d3a-810f-10240260f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Grid Search + CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 05:36:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/02/15 05:36:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "26/02/15 05:36:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search + CV completado en 59.39 segundos\n",
      "Mejor AUC Test (CV): 0.8262\n"
     ]
    }
   ],
   "source": [
    "cv_grid = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando Grid Search + CV...\")\n",
    "start_time = time.time()\n",
    "cv_grid_model = cv_grid.fit(train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "best_grid_model = cv_grid_model.bestModel\n",
    "auc_grid = evaluator.evaluate(best_grid_model.transform(test))\n",
    "\n",
    "print(f\"Grid Search + CV completado en {grid_time:.2f} segundos\")\n",
    "print(f\"Mejor AUC Test (CV): {auc_grid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaad9c2-c919-450a-90a6-8ec5afebe1ef",
   "metadata": {},
   "source": [
    "## RETO 3: Train-Validation Split (TVS)\n",
    "**Concepto**: A diferencia de CV, TVS solo entrena cada combinación 1 vez.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f09e537-544e-40ae-9f1d-dad7809b6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con Train-Validation Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVS completado en 17.63 segundos\n",
      "Mejor AUC Test (TVS): 0.8262\n"
     ]
    }
   ],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8, # 80% entrenamiento, 20% validación interna\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando con Train-Validation Split...\")\n",
    "start_time = time.time()\n",
    "tvs_model = tvs.fit(train)\n",
    "tvs_time = time.time() - start_time\n",
    "\n",
    "best_tvs_model = tvs_model.bestModel\n",
    "auc_tvs = evaluator.evaluate(best_tvs_model.transform(test))\n",
    "\n",
    "print(f\"TVS completado en {tvs_time:.2f} segundos\")\n",
    "print(f\"Mejor AUC Test (TVS): {auc_tvs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ddd44-a1a2-4f39-be82-97ae7b821744",
   "metadata": {},
   "source": [
    "\n",
    " ## RETO 4: Comparar Estrategias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd4d7d1-4307-4e85-9a09-d627320372da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "COMPARACIÓN DE ESTRATEGIAS\n",
      "========================================\n",
      "Grid Search + CV:  Tiempo: 59.4s | AUC: 0.8262\n",
      "TVS:               Tiempo: 17.6s  | AUC: 0.8262\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPARACIÓN DE ESTRATEGIAS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Grid Search + CV:  Tiempo: {grid_time:.1f}s | AUC: {auc_grid:.4f}\")\n",
    "print(f\"TVS:               Tiempo: {tvs_time:.1f}s  | AUC: {auc_tvs:.4f}\")\n",
    "\n",
    "# Respuesta: TVS es notablemente más rápido. Si el AUC es similar, TVS es mejor para Big Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e59763-7d07-4296-a480-a587036c60fd",
   "metadata": {},
   "source": [
    " ## RETO 5: Seleccionar y Guardar Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943f8456-4ee1-4c3c-8320-54b6130a6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo guardado. Estrategia ganadora: Grid Search + CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Grid Fino: 0.8262\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos el que tenga mayor AUC (mayor es mejor en clasificación)\n",
    "mejor_modelo = best_grid_model if auc_grid > auc_tvs else best_tvs_model\n",
    "model_path = \"/opt/spark-data/processed/tuned_logistic_model\"\n",
    "mejor_modelo.write().overwrite().save(model_path)\n",
    "\n",
    "# Guardar hiperparámetros\n",
    "hiperparametros_optimos = {\n",
    "    \"regParam\": float(mejor_modelo.getRegParam()),\n",
    "    \"elasticNetParam\": float(mejor_modelo.getElasticNetParam()),\n",
    "    \"maxIter\": int(mejor_modelo.getMaxIter()),\n",
    "    \"auc_test\": float(max(auc_grid, auc_tvs)),\n",
    "    \"estrategia\": \"Grid Search + CV\" if auc_grid > auc_tvs else \"TVS\"\n",
    "}\n",
    "\n",
    "with open(\"/opt/spark-data/processed/hiperparametros_optimos.json\", \"w\") as f:\n",
    "    json.dump(hiperparametros_optimos, f, indent=2)\n",
    "\n",
    "print(f\"Mejor modelo guardado. Estrategia ganadora: {hiperparametros_optimos['estrategia']}\")\n",
    "\n",
    "\n",
    "# ## RETO BONUS: Grid Más Fino\n",
    "# Si el mejor regParam fue 0.01, probaremos valores muy cercanos.\n",
    "\n",
    "fino_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.005, 0.01, 0.02]) \\\n",
    "    .addGrid(lr.elasticNetParam, [mejor_modelo.getElasticNetParam()]) \\\n",
    "    .build()\n",
    "\n",
    "cv_fino = CrossValidator(estimator=lr, estimatorParamMaps=fino_grid, evaluator=evaluator, numFolds=3)\n",
    "cv_fino_model = cv_fino.fit(train)\n",
    "print(f\"AUC Grid Fino: {evaluator.evaluate(cv_fino_model.bestModel.transform(test)):.4f}\")\n",
    "\n",
    "# ## Preguntas de Reflexión\n",
    "# 1. **Grid vs Random Search**: Grid Search es exhaustivo pero lento; Random Search es mejor si tienes muchísimos parámetros y quieres explorar rápido.\n",
    "# 2. **¿Por qué TVS es más rápido?**: Porque evita las rotaciones del K-Fold; solo entrena 1 vez por combinación.\n",
    "# 3. **Grid demasiado grande**: El tiempo de entrenamiento crece exponencialmente, pudiendo colapsar el clúster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce4ab5-a5c2-438a-b2a3-d01328e6ba40",
   "metadata": {},
   "source": [
    "el Train-Validation Split (TVS) es el claro ganador en términos de eficiencia. Logró exactamente el mismo AUC (0.8262) que la búsqueda exhaustiva (Grid Search + CV), pero en solo 17.6 segundos. Sin embargo, el valor de CV es un promedio de múltiples pruebas, esto hace que el modelo de Cross-Validation sea el \"ganador\".\n",
    "el Grid Fino confirmó que el modelo ya llegó a su techo de rendimiento con un AUC de 0.8262."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
