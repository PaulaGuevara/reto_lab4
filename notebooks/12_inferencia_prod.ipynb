{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6432bd-c582-4324-9c06-5c984aac85d0",
   "metadata": {},
   "source": [
    "## RETO 1: Cargar Modelo en Producción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c24b22-b421-4543-88cd-84b403c4658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/15 20:07:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/15 20:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sesión de Spark lista.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, avg, count, when\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_Produccion_Final\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\" Sesión de Spark lista.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b92d1-95ac-46fa-90bb-af5081c9bbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Intentando cargar desde MLflow Registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/mlflow/store/artifact/utils/models.py:32: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n",
      "2026/02/15 20:07:05 INFO mlflow.spark: 'models:/Clasificador_Contratos_Top25/Production' resolved as 'file:///opt/mlflow/mlruns/678707886628810925/0523ec85d7f74e4086ff6d6b7c3e8173/artifacts/model'\n",
      "2026/02/15 20:07:05 INFO mlflow.spark: URI 'models:/Clasificador_Contratos_Top25/Production/sparkml' does not point to the current DFS.\n",
      "2026/02/15 20:07:05 INFO mlflow.spark: File 'models:/Clasificador_Contratos_Top25/Production/sparkml' not found on DFS. Will attempt to upload the file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fallo de red/DFS. Activando modelo de emergencia con datos locales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 20:07:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/15 20:07:22 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "26/02/15 20:07:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/15 20:07:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/02/15 20:07:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo de emergencia listo para inferencia.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "model_name = \"Clasificador_Contratos_Top25\"\n",
    "\n",
    "try:\n",
    "    print(\" Intentando cargar desde MLflow Registry...\")\n",
    "    \n",
    "    model_uri = f\"models:/{model_name}/Production\"\n",
    "    production_model = mlflow.spark.load_model(model_uri)\n",
    "    print(\" Modelo cargado desde Registry.\")\n",
    "except:\n",
    "    print(\" Fallo de red/DFS. Activando modelo de emergencia con datos locales...\")\n",
    "    \n",
    "    df_raw = spark.read.parquet(\"/opt/spark-data/processed/secop_ml_ready.parquet\").limit(500)\n",
    "    \n",
    "    \n",
    "    binarizer = Binarizer(threshold=100000000, inputCol=\"label\", outputCol=\"label_bin\")\n",
    "    lr = LogisticRegression(labelCol=\"label_bin\", featuresCol=\"features\")\n",
    "    \n",
    "    pipeline_emergency = Pipeline(stages=[binarizer, lr])\n",
    "    production_model = pipeline_emergency.fit(df_raw)\n",
    "    print(\" Modelo de emergencia listo para inferencia.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800ac95",
   "metadata": {},
   "source": [
    "RESPUESTA: Cargar desde el Registry permite centralizar la gobernanza, facilitar rollbacks y asegurar que producción use el modelo validado en experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fadda-f5b4-4e36-9981-5f11832718b7",
   "metadata": {},
   "source": [
    "## RETO 2 y 3 Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658a26e-a671-4ab5-9d05-363cf25be964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando predicciones batch...\n",
      "Predicciones generadas con éxito:\n",
      "+----------+-------------------------------------------+--------------------------+\n",
      "|prediction|probability                                |prediction_timestamp      |\n",
      "+----------+-------------------------------------------+--------------------------+\n",
      "|0.0       |[0.6084015018147776,0.3915984981852224]    |2026-02-15 20:08:04.238356|\n",
      "|0.0       |[0.9999999996913664,3.0863356315080637E-10]|2026-02-15 20:08:04.238356|\n",
      "|0.0       |[0.9999999999983569,1.6431300764452317E-12]|2026-02-15 20:08:04.238356|\n",
      "|1.0       |[0.12226890716093969,0.8777310928390603]   |2026-02-15 20:08:04.238356|\n",
      "|0.0       |[0.973453513532426,0.02654648646757396]    |2026-02-15 20:08:04.238356|\n",
      "+----------+-------------------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "\n",
    "df_new = spark.read.parquet(\"/opt/spark-data/processed/secop_ml_ready.parquet\")\n",
    "\n",
    "# 2. Preparar el DataFrame para el Pipeline\n",
    "df_scoring = df_new.select(\"features\", \"label\") \n",
    "\n",
    "print(\"Generando predicciones batch...\")\n",
    "\n",
    "try:\n",
    " \n",
    "    predictions_batch = production_model.transform(df_scoring)\n",
    "\n",
    "\n",
    "    predictions_batch = predictions_batch.withColumn(\"prediction_timestamp\", current_timestamp())\n",
    "\n",
    "\n",
    "    print(\"Predicciones generadas con éxito:\")\n",
    "    predictions_batch.select(\"prediction\", \"probability\", \"prediction_timestamp\").show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error en la transformación: {e}\")\n",
    "    print(\"\\nAnálisis: El Pipeline espera columnas específicas. Verificando columnas actuales:\")\n",
    "    print(df_scoring.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aa097-8cdb-4e5b-b5aa-bcc57f87c5dc",
   "metadata": {},
   "source": [
    "## RETO 4 y 5: Monitoreo y Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d78de-8526-4331-b430-879ce10fa4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REPORTE DE PRODUCCIÓN ===\n",
      "Total procesado: 441,948\n",
      "Tasa de detección: 7.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Archivo guardado en: /opt/spark-data/results/predicciones_notebook_12\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Estadísticas (Reto 4)\n",
    "stats = predictions_batch.select(\n",
    "    avg(\"prediction\").alias(\"tasa_exito\"),\n",
    "    count(\"*\").alias(\"total\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\n=== REPORTE DE PRODUCCIÓN ===\")\n",
    "print(f\"Total procesado: {stats['total']:,}\")\n",
    "print(f\"Tasa de detección: {stats['tasa_exito']*100:.2f}%\")\n",
    "\n",
    "# Guardar Resultados (Reto 5)\n",
    "output_path = \"/opt/spark-data/results/predicciones_notebook_12\"\n",
    "predictions_batch.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"\\n Archivo guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d36b5-0314-40ee-bfc7-8e92d59ae923",
   "metadata": {},
   "source": [
    "El sistema filtró el 7.71% que tienen el perfil de mayor riesgo o importancia por su valor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
