{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b96516-2084-454f-9a56-c65c8d1ac528",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook 11: Model Registry con MLflow\n",
    "**Objetivo**: Gestionar el ciclo de vida del modelo SECOP (Versiones y Stages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227e7123-7131-4630-a344-04e2c1b36573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/15 19:49:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql.functions import col, when\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# %%\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_ModelRegistry\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dcb49-82ca-4eac-a463-845b66d9b68d",
   "metadata": {},
   "source": [
    "## RETO 1: Configurar MLflow y el Registry\n",
    "**Diferencia**: Tracking registra \"intentos\" (fotos), Registry gestiona \"productos\" (versiones oficiales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f7628-7b07-4e33-9c86-a413a53e2f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "client = MlflowClient()\n",
    "model_name = \"Clasificador_Contratos_Top25\"\n",
    "\n",
    "# Carga de datos\n",
    "df_raw = spark.read.parquet(\"/opt/spark-data/processed/secop_ml_ready.parquet\")\n",
    "discretizer = QuantileDiscretizer(numBuckets=4, inputCol=\"label\", outputCol=\"cuartil\")\n",
    "df_final = discretizer.fit(df_raw).transform(df_raw) \\\n",
    "    .withColumn(\"label\", when(col(\"cuartil\") == 3.0, 1.0).otherwise(0.0)) \\\n",
    "    .select(\"features\", \"label\")\n",
    "\n",
    "train, test = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e79ea-3b74-44af-9052-c1c8467126df",
   "metadata": {},
   "source": [
    "## RETO 2: Entrenar y Registrar Modelo v1 (Baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffde039-2899-4de5-938d-1c41c5062832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/15 19:49:41 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "26/02/15 19:49:45 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/02/15 19:49:45 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "26/02/15 19:49:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "2026/02/15 19:49:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpazez23qc/model, flavor: spark), fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback.\n",
      "/usr/local/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Registered model 'Clasificador_Contratos_Top25' already exists. Creating a new version of this model...\n",
      "2026/02/15 19:49:56 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Clasificador_Contratos_Top25, version 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 Registrada. AUC: 0.8260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '19' of model 'Clasificador_Contratos_Top25'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"/SECOP_Model_Registry\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Run_v1_Baseline\") as run:\n",
    "    lr_v1 = LogisticRegression(regParam=0.1, labelCol=\"label\")\n",
    "    model_v1 = lr_v1.fit(train)\n",
    "    auc_v1 = evaluator.evaluate(model_v1.transform(test))\n",
    "    \n",
    "    mlflow.log_metric(\"auc\", auc_v1)\n",
    "    \n",
    "    # Registro automático en el Model Registry\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=model_v1,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    print(f\"v1 Registrada. AUC: {auc_v1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568148f0-7d0e-4530-92eb-c1947ffb6517",
   "metadata": {},
   "source": [
    "\n",
    "## RETO 3: Entrenar y Registrar Modelo v2 (Optimizado)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eed5ef-3636-4f30-8e02-b42567f2d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/15 19:50:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpku0rk0oa/model, flavor: spark), fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback.\n",
      "Registered model 'Clasificador_Contratos_Top25' already exists. Creating a new version of this model...\n",
      "2026/02/15 19:50:05 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Clasificador_Contratos_Top25, version 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 Registrada. AUC: 0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '20' of model 'Clasificador_Contratos_Top25'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Run_v2_Optimizado\") as run:\n",
    "    lr_v2 = LogisticRegression(regParam=0.01, labelCol=\"label\")\n",
    "    model_v2 = lr_v2.fit(train)\n",
    "    auc_v2 = evaluator.evaluate(model_v2.transform(test))\n",
    "    \n",
    "    mlflow.log_metric(\"auc\", auc_v2)\n",
    "    \n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=model_v2,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    print(f\"v2 Registrada. AUC: {auc_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5b5ea-7104-4db6-beeb-1565f6bf9c7b",
   "metadata": {},
   "source": [
    "\n",
    "## RETO 4: Gestionar Versiones y Stages\n",
    "**Pregunta**: ¿Por qué Staging? \n",
    "**Respuesta**: Para realizar pruebas de integración (ver si el modelo carga bien \n",
    " en la API) antes de afectar a los usuarios reales en Production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82ad813-fb64-438f-aeae-8adfdcae091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión: 20, Stage: None\n",
      "Versión: 19, Stage: None\n",
      "Versión: 2, Stage: Production\n",
      "Versión: 1, Stage: Archived\n",
      "Versión: 18, Stage: None\n",
      "Versión: 17, Stage: None\n",
      "Versión: 16, Stage: None\n",
      "Versión: 15, Stage: None\n",
      "Versión: 14, Stage: None\n",
      "Versión: 13, Stage: None\n",
      "Versión: 12, Stage: None\n",
      "Versión: 11, Stage: None\n",
      "Versión: 10, Stage: None\n",
      "Versión: 9, Stage: None\n",
      "Versión: 8, Stage: None\n",
      "Versión: 7, Stage: None\n",
      "Versión: 6, Stage: None\n",
      "Versión: 5, Stage: None\n",
      "Versión: 4, Stage: None\n",
      "Versión: 3, Stage: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12904/3247327629.py:7: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name, version=2, stage=\"Production\")\n",
      "/tmp/ipykernel_12904/3247327629.py:8: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name, version=1, stage=\"Archived\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flujo de estados completado: v2 -> Production, v1 -> Archived\n"
     ]
    }
   ],
   "source": [
    "# Listar versiones\n",
    "versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "for v in versions:\n",
    "    print(f\"Versión: {v.version}, Stage: {v.current_stage}\")\n",
    "\n",
    "# Promover v2 a Production y Archivar v1\n",
    "client.transition_model_version_stage(name=model_name, version=2, stage=\"Production\")\n",
    "client.transition_model_version_stage(name=model_name, version=1, stage=\"Archived\")\n",
    "\n",
    "print(\"Flujo de estados completado: v2 -> Production, v1 -> Archived\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b67372-cbe6-44e7-a431-2f6b33fc2a46",
   "metadata": {},
   "source": [
    "\n",
    "## RETO 5: Agregar Metadata al Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b128a2-39fe-4816-9407-f0eaed4c0585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1771181286413, current_stage='Production', description=('Modelo optimizado con Ridge (regParam=0.01). AUC validado: 0.8262. Dataset '\n",
       " 'SECOP 2026.'), last_updated_timestamp=1771185006714, name='Clasificador_Contratos_Top25', run_id='0523ec85d7f74e4086ff6d6b7c3e8173', run_link='', source='file:///opt/mlflow/mlruns/678707886628810925/0523ec85d7f74e4086ff6d6b7c3e8173/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='2'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=2,\n",
    "    description=f\"Modelo optimizado con Ridge (regParam=0.01). AUC validado: {auc_v2:.4f}. Dataset SECOP 2026.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a5457-62b9-471c-a4d5-ec298b65cad2",
   "metadata": {},
   "source": [
    "\n",
    "## Reto 6: Reactivación SparkContext y Predicción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadb6e5-eaa3-46c8-8e3d-f15494d14c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkContext reactivado correctamente.\n",
      " Generando predicciones finales...\n",
      "+-----+----------------------------------------+----------+\n",
      "|label|probability                             |prediction|\n",
      "+-----+----------------------------------------+----------+\n",
      "|0.0  |[0.6182985454953545,0.38170145450464554]|0.0       |\n",
      "|0.0  |[0.6182985454953545,0.38170145450464554]|0.0       |\n",
      "|0.0  |[0.6182985454953545,0.38170145450464554]|0.0       |\n",
      "|0.0  |[0.6182985454953545,0.38170145450464554]|0.0       |\n",
      "|0.0  |[0.6182985454953545,0.38170145450464554]|0.0       |\n",
      "+-----+----------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 20:07:48 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "26/02/15 20:07:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if sc is not None:\n",
    "    print(\" SparkContext reactivado correctamente.\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Aseguramos que el modelo v2 esté listo\n",
    "        print(\" Generando predicciones finales...\")\n",
    "\n",
    "        demo_data = test.limit(5)\n",
    "        final_predictions = model_v2.transform(demo_data)\n",
    "\n",
    "        final_predictions.select(\"label\", \"probability\", \"prediction\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error en la transformación: {e}\")\n",
    "        print(\"Sugerencia: Si el error persiste, vuelve a ejecutar la celda donde entrenaste 'model_v2'.\")\n",
    "else:\n",
    "    print(\"No se pudo activar el SparkContext. Reinicia el Kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e103d1-dec2-45e0-bc5b-f58ac46dfdf4",
   "metadata": {},
   "source": [
    "Resultado 0.0: Significa que para esos 5 ejemplos, el modelo predice que NO pertenecen al Top 25% de los contratos más caros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
