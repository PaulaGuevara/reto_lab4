{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6eef5f-489c-4187-8cb4-b34f8ae2df23",
   "metadata": {},
   "source": [
    "### Configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ac0d45-faf4-4775-b6ad-b5b41c46e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-df371132-736d-4239-86f7-e2f650d4d9c3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 155ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-df371132-736d-4239-86f7-e2f650d4d9c3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "26/01/31 00:05:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, TimestampType ,DateType\n",
    "\n",
    "\n",
    "# Forzamos la limpieza de memoria en el driver antes de la escritura\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Lab_SECOP_Silver\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7df29-b1e6-430a-b7e2-5eef7b854d45",
   "metadata": {},
   "source": [
    "### Leer datos de Bronce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196ec6c2-81c8-4705-8d59-53d09fe1fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = \"/app/data/lakehouse/bronze/secop\"\n",
    "df_bronze= spark.read.format(\"delta\").load(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0841567a-e3f0-45c0-9907-d62d2ff147b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 00:05:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anno_bpin</th>\n",
       "      <th>c_digo_bpin</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>codigo_de_categoria_principal</th>\n",
       "      <th>codigo_entidad</th>\n",
       "      <th>codigo_proveedor</th>\n",
       "      <th>condiciones_de_entrega</th>\n",
       "      <th>departamento</th>\n",
       "      <th>descripcion_del_proceso</th>\n",
       "      <th>descripcion_documentos_tipo</th>\n",
       "      <th>...</th>\n",
       "      <th>valor_amortizado</th>\n",
       "      <th>valor_de_pago_adelantado</th>\n",
       "      <th>valor_del_contrato</th>\n",
       "      <th>valor_facturado</th>\n",
       "      <th>valor_pagado</th>\n",
       "      <th>valor_pendiente_de</th>\n",
       "      <th>valor_pendiente_de_ejecucion</th>\n",
       "      <th>valor_pendiente_de_pago</th>\n",
       "      <th>_ingestion_time</th>\n",
       "      <th>_source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>202500000002779</td>\n",
       "      <td>Chinácota</td>\n",
       "      <td>V1.80111600</td>\n",
       "      <td>704851104</td>\n",
       "      <td>730599727</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Norte de Santander</td>\n",
       "      <td>PRESTAR LOS SERVICIOS PROFESIONALES COMO FORMA...</td>\n",
       "      <td>No definido</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12343333</td>\n",
       "      <td>12343333</td>\n",
       "      <td>11500000</td>\n",
       "      <td>0</td>\n",
       "      <td>843333</td>\n",
       "      <td>843333</td>\n",
       "      <td>2026-01-30 22:18:40.643078</td>\n",
       "      <td>API_Socrata_Bogota_2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No D</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>V1.80111607</td>\n",
       "      <td>705008498</td>\n",
       "      <td>718547920</td>\n",
       "      <td>Como acordado previamente</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Brindar acompañamiento jurídico y apoyo profes...</td>\n",
       "      <td>No definido</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20000000</td>\n",
       "      <td>20000000</td>\n",
       "      <td>20000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-30 22:18:40.643078</td>\n",
       "      <td>API_Socrata_Bogota_2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No D</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Cartagena</td>\n",
       "      <td>V1.85121600</td>\n",
       "      <td>709192637</td>\n",
       "      <td>712389626</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Bolívar</td>\n",
       "      <td>PRESTACIÓN DE LOS SERVICIOS COMO GESTORES DE S...</td>\n",
       "      <td>No definido</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4000000</td>\n",
       "      <td>4000000</td>\n",
       "      <td>4000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-30 22:18:40.643078</td>\n",
       "      <td>API_Socrata_Bogota_2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No D</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Los Patios</td>\n",
       "      <td>V1.85101601</td>\n",
       "      <td>713088169</td>\n",
       "      <td>731045159</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Norte de Santander</td>\n",
       "      <td>PRESTAR SUS SERVICIOS EN CONDICIÓN DE TÉCNICO ...</td>\n",
       "      <td>No definido</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026-01-30 22:18:40.643078</td>\n",
       "      <td>API_Socrata_Bogota_2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No D</td>\n",
       "      <td>No Definido</td>\n",
       "      <td>Amalfi</td>\n",
       "      <td>V1.81111819</td>\n",
       "      <td>718317027</td>\n",
       "      <td>718871106</td>\n",
       "      <td>A convenir</td>\n",
       "      <td>Antioquia</td>\n",
       "      <td>Prestación de servicios de Asesoría en Calidad...</td>\n",
       "      <td>No definido</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12637800</td>\n",
       "      <td>8425200</td>\n",
       "      <td>8425200</td>\n",
       "      <td>0</td>\n",
       "      <td>4212600</td>\n",
       "      <td>4212600</td>\n",
       "      <td>2026-01-30 22:18:40.643078</td>\n",
       "      <td>API_Socrata_Bogota_2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  anno_bpin      c_digo_bpin       ciudad codigo_de_categoria_principal  \\\n",
       "0      2025  202500000002779    Chinácota                   V1.80111600   \n",
       "1      No D      No Definido  No Definido                   V1.80111607   \n",
       "2      No D      No Definido    Cartagena                   V1.85121600   \n",
       "3      No D      No Definido   Los Patios                   V1.85101601   \n",
       "4      No D      No Definido       Amalfi                   V1.81111819   \n",
       "\n",
       "  codigo_entidad codigo_proveedor     condiciones_de_entrega  \\\n",
       "0      704851104        730599727                No Definido   \n",
       "1      705008498        718547920  Como acordado previamente   \n",
       "2      709192637        712389626                No Definido   \n",
       "3      713088169        731045159                No Definido   \n",
       "4      718317027        718871106                 A convenir   \n",
       "\n",
       "         departamento                            descripcion_del_proceso  \\\n",
       "0  Norte de Santander  PRESTAR LOS SERVICIOS PROFESIONALES COMO FORMA...   \n",
       "1         No Definido  Brindar acompañamiento jurídico y apoyo profes...   \n",
       "2             Bolívar  PRESTACIÓN DE LOS SERVICIOS COMO GESTORES DE S...   \n",
       "3  Norte de Santander  PRESTAR SUS SERVICIOS EN CONDICIÓN DE TÉCNICO ...   \n",
       "4           Antioquia  Prestación de servicios de Asesoría en Calidad...   \n",
       "\n",
       "  descripcion_documentos_tipo  ... valor_amortizado valor_de_pago_adelantado  \\\n",
       "0                 No definido  ...                0                        0   \n",
       "1                 No definido  ...                0                        0   \n",
       "2                 No definido  ...                0                        0   \n",
       "3                 No definido  ...                0                        0   \n",
       "4                 No definido  ...                0                        0   \n",
       "\n",
       "  valor_del_contrato valor_facturado valor_pagado valor_pendiente_de  \\\n",
       "0           12343333        12343333     11500000                  0   \n",
       "1           20000000        20000000     20000000                  0   \n",
       "2            4000000         4000000      4000000                  0   \n",
       "3           10000000        10000000     10000000                  0   \n",
       "4           12637800         8425200      8425200                  0   \n",
       "\n",
       "  valor_pendiente_de_ejecucion valor_pendiente_de_pago  \\\n",
       "0                       843333                  843333   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                      4212600                 4212600   \n",
       "\n",
       "             _ingestion_time             _source_file  \n",
       "0 2026-01-30 22:18:40.643078  API_Socrata_Bogota_2025  \n",
       "1 2026-01-30 22:18:40.643078  API_Socrata_Bogota_2025  \n",
       "2 2026-01-30 22:18:40.643078  API_Socrata_Bogota_2025  \n",
       "3 2026-01-30 22:18:40.643078  API_Socrata_Bogota_2025  \n",
       "4 2026-01-30 22:18:40.643078  API_Socrata_Bogota_2025  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bronze.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e604a7c-15cc-4037-8828-a544cfd84cbc",
   "metadata": {},
   "source": [
    "### Transformación del tipo de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add6069e-ba12-434b-a457-e7e5c23df7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 00:06:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipado completo. Revisando esquema...\n",
      "root\n",
      " |-- anno_bpin: string (nullable = true)\n",
      " |-- c_digo_bpin: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- codigo_de_categoria_principal: string (nullable = true)\n",
      " |-- codigo_entidad: long (nullable = true)\n",
      " |-- codigo_proveedor: string (nullable = true)\n",
      " |-- condiciones_de_entrega: string (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- descripcion_del_proceso: string (nullable = true)\n",
      " |-- descripcion_documentos_tipo: string (nullable = true)\n",
      " |-- destino_gasto: string (nullable = true)\n",
      " |-- dias_adicionados: long (nullable = true)\n",
      " |-- documento_proveedor: string (nullable = true)\n",
      " |-- documentos_tipo: string (nullable = true)\n",
      " |-- domicilio_representante_legal: string (nullable = true)\n",
      " |-- duraci_n_del_contrato: string (nullable = true)\n",
      " |-- el_contrato_puede_ser_prorrogado: string (nullable = true)\n",
      " |-- entidad_centralizada: string (nullable = true)\n",
      " |-- es_grupo: string (nullable = true)\n",
      " |-- es_pyme: string (nullable = true)\n",
      " |-- espostconflicto: string (nullable = true)\n",
      " |-- estado_bpin: string (nullable = true)\n",
      " |-- estado_contrato: string (nullable = true)\n",
      " |-- fecha_de_fin_del_contrato: timestamp (nullable = true)\n",
      " |-- fecha_de_firma: timestamp (nullable = true)\n",
      " |-- fecha_de_inicio_del_contrato: timestamp (nullable = true)\n",
      " |-- fecha_de_notificaci_n_de_prorrogaci_n: timestamp (nullable = true)\n",
      " |-- fecha_fin_liquidacion: timestamp (nullable = true)\n",
      " |-- fecha_inicio_liquidacion: timestamp (nullable = true)\n",
      " |-- g_nero_representante_legal: string (nullable = true)\n",
      " |-- habilita_pago_adelantado: string (nullable = true)\n",
      " |-- id_contrato: string (nullable = true)\n",
      " |-- identificaci_n_representante_legal: string (nullable = true)\n",
      " |-- justificacion_modalidad_de: string (nullable = true)\n",
      " |-- liquidaci_n: string (nullable = true)\n",
      " |-- localizaci_n: string (nullable = true)\n",
      " |-- modalidad_de_contratacion: string (nullable = true)\n",
      " |-- n_mero_de_cuenta: string (nullable = true)\n",
      " |-- n_mero_de_documento_ordenador_de_pago: string (nullable = true)\n",
      " |-- n_mero_de_documento_ordenador_del_gasto: string (nullable = true)\n",
      " |-- n_mero_de_documento_supervisor: string (nullable = true)\n",
      " |-- nacionalidad_representante_legal: string (nullable = true)\n",
      " |-- nit_entidad: long (nullable = true)\n",
      " |-- nombre_del_banco: string (nullable = true)\n",
      " |-- nombre_entidad: string (nullable = true)\n",
      " |-- nombre_ordenador_de_pago: string (nullable = true)\n",
      " |-- nombre_ordenador_del_gasto: string (nullable = true)\n",
      " |-- nombre_representante_legal: string (nullable = true)\n",
      " |-- nombre_supervisor: string (nullable = true)\n",
      " |-- objeto_del_contrato: string (nullable = true)\n",
      " |-- obligaci_n_ambiental: string (nullable = true)\n",
      " |-- obligaciones_postconsumo: string (nullable = true)\n",
      " |-- orden: string (nullable = true)\n",
      " |-- origen_de_los_recursos: string (nullable = true)\n",
      " |-- pilares_del_acuerdo: string (nullable = true)\n",
      " |-- presupuesto_general_de_la_nacion_pgn: decimal(18,2) (nullable = true)\n",
      " |-- proceso_de_compra: string (nullable = true)\n",
      " |-- proveedor_adjudicado: string (nullable = true)\n",
      " |-- puntos_del_acuerdo: string (nullable = true)\n",
      " |-- rama: string (nullable = true)\n",
      " |-- recursos_de_credito: decimal(18,2) (nullable = true)\n",
      " |-- recursos_propios: decimal(18,2) (nullable = true)\n",
      " |-- recursos_propios_alcald_as_gobernaciones_y_resguardos_ind_genas_: decimal(18,2) (nullable = true)\n",
      " |-- referencia_del_contrato: string (nullable = true)\n",
      " |-- reversion: string (nullable = true)\n",
      " |-- saldo_cdp: decimal(18,2) (nullable = true)\n",
      " |-- saldo_vigencia: decimal(18,2) (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- sistema_general_de_participaciones: decimal(18,2) (nullable = true)\n",
      " |-- sistema_general_de_regal_as: decimal(18,2) (nullable = true)\n",
      " |-- tipo_de_contrato: string (nullable = true)\n",
      " |-- tipo_de_cuenta: string (nullable = true)\n",
      " |-- tipo_de_documento_ordenador_de_pago: string (nullable = true)\n",
      " |-- tipo_de_documento_ordenador_del_gasto: string (nullable = true)\n",
      " |-- tipo_de_documento_supervisor: string (nullable = true)\n",
      " |-- tipo_de_identificaci_n_representante_legal: string (nullable = true)\n",
      " |-- tipodocproveedor: string (nullable = true)\n",
      " |-- ultima_actualizacion: timestamp (nullable = true)\n",
      " |-- urlproceso: string (nullable = true)\n",
      " |-- valor_amortizado: decimal(18,2) (nullable = true)\n",
      " |-- valor_de_pago_adelantado: decimal(18,2) (nullable = true)\n",
      " |-- valor_del_contrato: decimal(18,2) (nullable = true)\n",
      " |-- valor_facturado: decimal(18,2) (nullable = true)\n",
      " |-- valor_pagado: decimal(18,2) (nullable = true)\n",
      " |-- valor_pendiente_de: decimal(18,2) (nullable = true)\n",
      " |-- valor_pendiente_de_ejecucion: decimal(18,2) (nullable = true)\n",
      " |-- valor_pendiente_de_pago: decimal(18,2) (nullable = true)\n",
      " |-- _ingestion_time: timestamp (nullable = true)\n",
      " |-- _source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grupos de columnas \n",
    "\n",
    "# Números (Money/Decimal)\n",
    "cols_decimal = [\n",
    "    \"valor_del_contrato\", \"valor_de_pago_adelantado\", \"valor_facturado\", \n",
    "    \"valor_pendiente_de_pago\", \"valor_pagado\", \"valor_amortizado\", \n",
    "    \"valor_pendiente_de\", \"valor_pendiente_de_ejecucion\", \"saldo_cdp\", \n",
    "    \"saldo_vigencia\", \"presupuesto_general_de_la_nacion_pgn\", \n",
    "    \"sistema_general_de_participaciones\", \"sistema_general_de_regal_as\",\n",
    "    \"recursos_propios_alcald_as_gobernaciones_y_resguardos_ind_genas_\",\n",
    "    \"recursos_de_credito\", \"recursos_propios\"\n",
    "]\n",
    "\n",
    "#  Fechas (Timestamp)\n",
    "cols_timestamp = [\n",
    "    \"fecha_de_firma\", \"fecha_de_inicio_del_contrato\", \"fecha_de_fin_del_contrato\",\n",
    "    \"ultima_actualizacion\", \"fecha_inicio_liquidacion\", \"fecha_fin_liquidacion\",\n",
    "    \"fecha_de_notificaci_n_de_prorrogaci_n\"\n",
    "]\n",
    "\n",
    "# números enteros o IDs\n",
    "cols_int = [\"nit_entidad\", \"codigo_entidad\", \"dias_adicionados\"]\n",
    "\n",
    "# 3. Aplicar las transformaciones\n",
    "df_silver_typed = df_bronze\n",
    "\n",
    "for c in df_bronze.columns:\n",
    "    if c in cols_decimal:\n",
    "        df_silver_typed = df_silver_typed.withColumn(c, F.col(c).cast(DecimalType(18, 2)))\n",
    "    elif c in cols_timestamp:\n",
    "        df_silver_typed = df_silver_typed.withColumn(c, F.col(c).cast(\"timestamp\"))\n",
    "    elif c in cols_int:\n",
    "        df_silver_typed = df_silver_typed.withColumn(c, F.col(c).cast(\"long\"))\n",
    "    # El resto se quedan como String automáticamente\n",
    "\n",
    "print(\"Tipado completo. Revisando esquema...\")\n",
    "df_silver_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbb73d-0711-481b-a915-1ca4410b7550",
   "metadata": {},
   "source": [
    "### Descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc07874-ca53-4186-a4be-3d6390b6c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumen_robusto_numericas(df, columnas):\n",
    "    resultados = []\n",
    "\n",
    "    for col in columnas:\n",
    "        # 1. Calculamos las estadísticas\n",
    "        stats = df.select(\n",
    "            F.count(F.col(col)).alias(\"n\"),\n",
    "            F.mean(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"missing_prop\"),\n",
    "            F.expr(f\"percentile_approx({col}, 0.5)\").alias(\"mediana\"),\n",
    "            F.expr(f\"percentile_approx({col}, 0.1)\").alias(\"p10\"),\n",
    "            F.expr(f\"percentile_approx({col}, 0.9)\").alias(\"p90\"),\n",
    "            (\n",
    "                F.expr(f\"percentile_approx({col}, 0.75)\") -\n",
    "                F.expr(f\"percentile_approx({col}, 0.25)\")\n",
    "            ).alias(\"iqr\"),\n",
    "            F.mean(F.when(F.col(col) == 0, 1).otherwise(0)).alias(\"ceros_prop\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        # 2. Convertimos TODO a float para evitar el error de mezcla de tipos\n",
    "        # Usamos float() para asegurar que Decimal y Long convivan en paz\n",
    "        resultados.append((\n",
    "            col,\n",
    "            int(stats[\"n\"]),\n",
    "            round(float(stats[\"missing_prop\"]) * 100, 2),\n",
    "            float(stats[\"mediana\"]) if stats[\"mediana\"] is not None else 0.0,\n",
    "            float(stats[\"p10\"]) if stats[\"p10\"] is not None else 0.0,\n",
    "            float(stats[\"p90\"]) if stats[\"p90\"] is not None else 0.0,\n",
    "            float(stats[\"iqr\"]) if stats[\"iqr\"] is not None else 0.0,\n",
    "            round(float(stats[\"ceros_prop\"]) * 100, 2)\n",
    "        ))\n",
    "\n",
    "    return spark.createDataFrame(\n",
    "        resultados,\n",
    "        [\n",
    "            \"variable\", \"n\", \"missing_%\", \"mediana\", \"p10\", \"p90\", \"iqr\", \"ceros_%\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615ee825-7bb4-47da-9af1-8aaea289d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_numericas = [\n",
    "    \"valor_del_contrato\", \n",
    "    \"valor_pagado\", \n",
    "    \"valor_facturado\", \n",
    "    \"dias_adicionados\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b2fa5f-22af-4fcd-b1ea-a9d9b22963c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+---------+-----------+---------+-----------+-----------+-------+\n",
      "|          variable|     n|missing_%|    mediana|      p10|        p90|        iqr|ceros_%|\n",
      "+------------------+------+---------+-----------+---------+-----------+-----------+-------+\n",
      "|valor_del_contrato|456617|      0.0|1.4225834E7|4000000.0|   1.1715E8|     2.45E7|   0.85|\n",
      "|      valor_pagado|456617|      0.0|        0.0|      0.0|1.6389504E7|  5962064.0|  66.74|\n",
      "|   valor_facturado|456617|      0.0|        0.0|      0.0|2.4533333E7|1.1187943E7|  51.36|\n",
      "|  dias_adicionados|456617|      0.0|        0.0|      0.0|       16.0|        0.0|  86.55|\n",
      "+------------------+------+---------+-----------+---------+-----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resumen_num = resumen_robusto_numericas(df_silver_typed, vars_numericas)\n",
    "resumen_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd0fd16-b96a-4f70-958c-442dd30d20a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando descriptivas categóricas...\n",
      "\n",
      " Top 5 para la columna: TIPO_DE_CONTRATO\n",
      "+-----------------------+------+----------+\n",
      "|tipo_de_contrato       |count |porcentaje|\n",
      "+-----------------------+------+----------+\n",
      "|Prestación de servicios|359310|78.69     |\n",
      "|Otro                   |26529 |5.81      |\n",
      "|Decreto 092 de 2017    |24869 |5.45      |\n",
      "|Compraventa            |14466 |3.17      |\n",
      "|Suministros            |14406 |3.15      |\n",
      "+-----------------------+------+----------+\n",
      "\n",
      "\n",
      " Top 5 para la columna: MODALIDAD_DE_CONTRATACION\n",
      "+------------------------------------+------+----------+\n",
      "|modalidad_de_contratacion           |count |porcentaje|\n",
      "+------------------------------------+------+----------+\n",
      "|Contratación directa                |313201|68.59     |\n",
      "|Contratación régimen especial       |76965 |16.86     |\n",
      "|Mínima cuantía                      |32876 |7.2       |\n",
      "|Selección Abreviada de Menor Cuantía|7677  |1.68      |\n",
      "|Selección abreviada subasta inversa |7623  |1.67      |\n",
      "+------------------------------------+------+----------+\n",
      "\n",
      "\n",
      " Top 5 para la columna: ESTADO_CONTRATO\n",
      "+---------------+------+----------+\n",
      "|estado_contrato|count |porcentaje|\n",
      "+---------------+------+----------+\n",
      "|En ejecución   |308634|67.59     |\n",
      "|Modificado     |97220 |21.29     |\n",
      "|terminado      |23144 |5.07      |\n",
      "|Aprobado       |12849 |2.81      |\n",
      "|Cerrado        |10169 |2.23      |\n",
      "+---------------+------+----------+\n",
      "\n",
      "\n",
      " Top 5 para la columna: SECTOR\n",
      "+-------------------------+------+----------+\n",
      "|sector                   |count |porcentaje|\n",
      "+-------------------------+------+----------+\n",
      "|Servicio Público         |131838|28.87     |\n",
      "|No aplica/No pertenece   |81027 |17.75     |\n",
      "|Salud y Protección Social|76591 |16.77     |\n",
      "|Educación Nacional       |23163 |5.07      |\n",
      "|deportes                 |18433 |4.04      |\n",
      "+-------------------------+------+----------+\n",
      "\n",
      "\n",
      " Top 5 para la columna: ORDEN\n",
      "+--------------------+------+----------+\n",
      "|orden               |count |porcentaje|\n",
      "+--------------------+------+----------+\n",
      "|Territorial         |330296|72.34     |\n",
      "|Nacional            |121261|26.56     |\n",
      "|Corporación Autónoma|5059  |1.11      |\n",
      "|No Definido         |1     |0.0       |\n",
      "+--------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RESUMEN CATEGORICAS\n",
    "\n",
    "def resumen_categoricas(df, columnas, top=5):\n",
    "    resultados = {}\n",
    "    n_total = df.count()\n",
    "\n",
    "    for col in columnas:\n",
    "        tabla = (\n",
    "            df.groupBy(col)\n",
    "            .count()\n",
    "            .withColumn(\"porcentaje\", F.round(F.col(\"count\") / n_total * 100, 2))\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "            .limit(top)\n",
    "        )\n",
    "        resultados[col] = tabla\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# variables de interés\n",
    "vars_categoricas = [\n",
    "    \"tipo_de_contrato\", \n",
    "    \"modalidad_de_contratacion\", \n",
    "    \"estado_contrato\",\n",
    "    \"sector\",\n",
    "    \"orden\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Generando descriptivas categóricas...\")\n",
    "resumen_cat = resumen_categoricas(df_silver_typed, vars_categoricas)\n",
    "\n",
    "# Imprimir los resultados\n",
    "for columna, tabla_frecuencia in resumen_cat.items():\n",
    "    print(f\"\\n Top 5 para la columna: {columna.upper()}\")\n",
    "    tabla_frecuencia.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db58f01-eab1-47b4-88dc-32c7d50b0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generando análisis temporal...\n",
      "\n",
      "=== RANGO CRONOLÓGICO DE LA DATA ===\n",
      "+-----------------------+-----------------------+--------------------+-------------------+\n",
      "|Primer_Contrato_Firmado|Ultimo_Contrato_Firmado|Inicio_Ejecucion_Min|  Fin_Ejecucion_Max|\n",
      "+-----------------------+-----------------------+--------------------+-------------------+\n",
      "|    2025-07-01 00:00:00|    2025-12-31 00:00:00| 1899-11-06 00:00:00|5025-12-20 00:00:00|\n",
      "+-----------------------+-----------------------+--------------------+-------------------+\n",
      "\n",
      "=== ESTADÍSTICAS DE DURACIÓN DE CONTRATOS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:==================>                                     (5 + 10) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+---------------+---------------+\n",
      "|Duracion_Promedio_Dias|Duracion_Mediana_Dias|Duracion_Minima|Duracion_Maxima|\n",
      "+----------------------+---------------------+---------------+---------------+\n",
      "|                110.34|                   90|        -730477|        1095834|\n",
      "+----------------------+---------------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. ANÁLISIS TEMPORAL (Fechas y Duración)\n",
    "\n",
    "print(\"\\n Generando análisis temporal...\")\n",
    "\n",
    "# Calculamos la duración en días (Fecha Fin - Fecha Inicio)\n",
    "\n",
    "df_tiempos = df_silver_typed.withColumn(\n",
    "    \"duracion_contrato_dias\",\n",
    "    F.datediff(F.col(\"fecha_de_fin_del_contrato\"), F.col(\"fecha_de_inicio_del_contrato\"))\n",
    ")\n",
    "\n",
    "# rango total de los datos\n",
    "rango_temporal = df_tiempos.select(\n",
    "    F.min(\"fecha_de_firma\").alias(\"Primer_Contrato_Firmado\"),\n",
    "    F.max(\"fecha_de_firma\").alias(\"Ultimo_Contrato_Firmado\"),\n",
    "    F.min(\"fecha_de_inicio_del_contrato\").alias(\"Inicio_Ejecucion_Min\"),\n",
    "    F.max(\"fecha_de_fin_del_contrato\").alias(\"Fin_Ejecucion_Max\")\n",
    ")\n",
    "\n",
    "# Calculamos la duración promedio y mediana de los contratos\n",
    "estadisticas_duracion = df_tiempos.select(\n",
    "    F.round(F.mean(\"duracion_contrato_dias\"), 2).alias(\"Duracion_Promedio_Dias\"),\n",
    "    F.expr(\"percentile_approx(duracion_contrato_dias, 0.5)\").alias(\"Duracion_Mediana_Dias\"),\n",
    "    F.min(\"duracion_contrato_dias\").alias(\"Duracion_Minima\"),\n",
    "    F.max(\"duracion_contrato_dias\").alias(\"Duracion_Maxima\")\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n=== RANGO CRONOLÓGICO DE LA DATA ===\")\n",
    "rango_temporal.show()\n",
    "\n",
    "print(\"=== ESTADÍSTICAS DE DURACIÓN DE CONTRATOS ===\")\n",
    "estadisticas_duracion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caadc0-6d1d-4332-8955-c721cff14fce",
   "metadata": {},
   "source": [
    "### Validación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6702ac9-92e4-4ec0-aad9-8a299b839a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros listos para Silver: 438143\n",
      "Registros para Cuarentena: 6412\n"
     ]
    }
   ],
   "source": [
    "# Calcular la duración del contrato mayor a 0\n",
    "df_with_duration = df_silver_typed.withColumn(\n",
    "    \"duracion_calculada\", \n",
    "    F.datediff(F.col(\"fecha_de_fin_del_contrato\"), F.col(\"fecha_de_inicio_del_contrato\"))\n",
    ")\n",
    "\n",
    "# límite cronológico (Año 1900 adelante)\n",
    "fecha_limite = \"1900-01-01\"\n",
    "\n",
    "# Identificar automáticamente todas las columnas de tipo fecha/timestamp\n",
    "cols_temporales = [f.name for f in df_with_duration.schema.fields \n",
    "                   if isinstance(f.dataType, (DateType, TimestampType))]\n",
    "\n",
    "# Reglas: \n",
    "# - Precio > 0\n",
    "# - Fecha de Firma no nula\n",
    "# - Duración > 0\n",
    "# - Orden diferente a \"No Definido\"\n",
    "\n",
    "\n",
    "# Construir la condición dinámica para TODAS las fechas\n",
    "# Genera: (F.col(fecha1) >= '1900-01-01') & (F.col(fecha2) >= '1900-01-01') \n",
    "\n",
    "\n",
    "condicion_fechas_ok = F.lit(True) # Punto de partida neutro para el operador &\n",
    "for col_name in cols_temporales:\n",
    "    condicion_fechas_ok &= (F.col(col_name) >= fecha_limite)\n",
    "\n",
    "# Condición de Validación (Quality Gate)\n",
    "condicion_valida = (\n",
    "    (F.col(\"valor_del_contrato\") > 0) & \n",
    "    (F.col(\"fecha_de_firma\").isNotNull()) &\n",
    "    (F.col(\"fecha_de_firma\") >= fecha_limite) &\n",
    "    (F.col(\"fecha_de_inicio_del_contrato\") >= fecha_limite) &\n",
    "    (F.col(\"fecha_de_fin_del_contrato\") >= fecha_limite) &\n",
    "    (F.col(\"duracion_calculada\") > 0) &\n",
    "    (F.col(\"orden\").isNotNull()) & \n",
    "    (F.upper(F.col(\"orden\")) != \"NO DEFINIDO\")\n",
    ")\n",
    "\n",
    "# BIFURCACIÓN (Split)\n",
    "df_silver = df_with_duration.filter(condicion_valida)\n",
    "\n",
    "# Registros Inválidos (Quarantine)\n",
    "df_quarantine = df_with_duration.filter(~condicion_valida)\n",
    "\n",
    "\n",
    "#  Asignación de Motivos de Rechazo Detallados\n",
    "df_quarantine = df_quarantine.withColumn(\n",
    "    \"motivo_rechazo\",\n",
    "    F.when(F.col(\"valor_del_contrato\") <= 0, \"Precio base menor o igual a 0\")\n",
    "     .when(F.col(\"fecha_de_firma\").isNull(), \"Fecha de firma nula\")\n",
    "     .when(F.col(\"fecha_de_firma\") < fecha_limite, \"Fecha de firma anterior a 1900\")\n",
    "     .when(F.col(\"fecha_de_inicio_del_contrato\") < fecha_limite, \"Fecha inicio anterior a 1900\")\n",
    "     .when(F.col(\"fecha_de_fin_del_contrato\") < fecha_limite, \"Fecha fin anterior a 1900\")\n",
    "     .when(F.col(\"duracion_calculada\") <= 0, \"Duración de contrato inválida (<= 0 días)\")\n",
    "     .when(F.col(\"duracion_calculada\").isNull(), \"Fechas inconsistentes para calcular duración\")\n",
    "     .when(F.upper(F.col(\"orden\")) == \"NO DEFINIDO\", \"Columna 'Orden' marcada como No Definido\")\n",
    "     .when(~condicion_fechas_ok, \"Alguna fecha es anterior a 1900 o inválida\")\n",
    "     .otherwise(\"Error de integridad o campos nulos\")\n",
    ")\n",
    "\n",
    "# numero de registros\n",
    "print(f\"Registros listos para Silver: {df_silver.count()}\")\n",
    "print(f\"Registros para Cuarentena: {df_quarantine.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6474e-0264-4efc-8175-8794d9e9f2f6",
   "metadata": {},
   "source": [
    "### Guardar Datos en Silver y Quarantine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80138454-5bcc-41fc-ba7b-012de9a7b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iniciando guardado de Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 00:06:33 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007c27506214c8.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "26/01/31 00:06:33 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007c27506214c8.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:501)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.newInvokeSpecial(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda/0x00007c27506214c8.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 5.0 in stage 86.0 (TID 1089)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 7.0 in stage 86.0 (TID 1091)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 9.0 in stage 86.0 (TID 1093)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 6.0 in stage 86.0 (TID 1090)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.math.BigInteger.valueOf(BigInteger.java:1209)\n",
      "\tat java.base/java.math.BigDecimal.inflated(BigDecimal.java:4521)\n",
      "\tat java.base/java.math.BigDecimal.unscaledValue(BigDecimal.java:2725)\n",
      "\tat org.apache.spark.sql.types.Decimal.toUnscaledLong(Decimal.scala:221)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:188)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda/0x00007c27515422f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007c2750f1bd58.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007c2750f0c7d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 8.0 in stage 86.0 (TID 1092)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.parseTimestampString(SparkDateTimeUtils.scala:408)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.parseTimestampString$(SparkDateTimeUtils.scala:394)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.parseTimestampString(DateTimeUtils.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToTimestamp(SparkDateTimeUtils.scala:546)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToTimestamp$(SparkDateTimeUtils.scala:544)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTimestamp(DateTimeUtils.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.stringToTimestamp(DateTimeUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda/0x00007c27515422f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007c2750f1bd58.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007c2750f0c7d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "26/01/31 00:06:33 ERROR Executor: Exception in task 4.0 in stage 86.0 (TID 1088)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#131,Executor task launch worker for task 9.0 in stage 86.0 (TID 1093),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#125,Executor task launch worker for task 4.0 in stage 86.0 (TID 1088),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#128,Executor task launch worker for task 5.0 in stage 86.0 (TID 1089),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[#130,Executor task launch worker for task 6.0 in stage 86.0 (TID 1090),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.math.BigInteger.valueOf(BigInteger.java:1209)\n",
      "\tat java.base/java.math.BigDecimal.inflated(BigDecimal.java:4521)\n",
      "\tat java.base/java.math.BigDecimal.unscaledValue(BigDecimal.java:2725)\n",
      "\tat org.apache.spark.sql.types.Decimal.toUnscaledLong(Decimal.scala:221)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:188)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda/0x00007c27515422f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007c2750f1bd58.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007c2750f0c7d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[#134,Executor task launch worker for task 7.0 in stage 86.0 (TID 1091),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "26/01/31 00:06:33 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[#104,Executor task launch worker for task 8.0 in stage 86.0 (TID 1092),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.parseTimestampString(SparkDateTimeUtils.scala:408)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.parseTimestampString$(SparkDateTimeUtils.scala:394)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.parseTimestampString(DateTimeUtils.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToTimestamp(SparkDateTimeUtils.scala:546)\n",
      "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToTimestamp$(SparkDateTimeUtils.scala:544)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTimestamp(DateTimeUtils.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.stringToTimestamp(DateTimeUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda/0x00007c27515422f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007c2750f1bd58.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x00007c2750f0c7d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "26/01/31 00:06:33 WARN TaskSetManager: Lost task 5.0 in stage 86.0 (TID 1089) (bd650bfca43a executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "26/01/31 00:06:33 ERROR TaskSetManager: Task 5 in stage 86.0 failed 1 times; aborting job\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51422)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j.reflection does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 3. Guardado de SILVER \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Iniciando guardado de Silver...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m (\u001b[43mdf_silver\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxRecordsPerFile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Evita archivos gigantes que consumen mucha RAM\u001b[39;49;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilver_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 4. Guardado de CUARENTENA\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Iniciando guardado de Cuarentena...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1461\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save()\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*a, **kw)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     converted = \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/delta/exceptions.py:160\u001b[39m, in \u001b[36m_patch_convert_exception.<locals>.convert_delta_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m delta_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_exception\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_convert_sql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:132\u001b[39m, in \u001b[36mconvert_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ParseException(origin=e)\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Order matters. ParseException inherits AnalysisException.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.AnalysisException\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AnalysisException(origin=e)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:464\u001b[39m, in \u001b[36mis_instance_of\u001b[39m\u001b[34m(gateway, java_object, java_class)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    462\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpy4j\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreflection\u001b[49m.TypeUtil.isInstanceOf(\n\u001b[32m    465\u001b[39m     param, java_object)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1664\u001b[39m, in \u001b[36mJavaPackage.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1661\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaClass(\n\u001b[32m   1662\u001b[39m         answer[proto.CLASS_FQN_START:], \u001b[38;5;28mself\u001b[39m._gateway_client)\n\u001b[32m   1663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m does not exist in the JVM\u001b[39m\u001b[33m\"\u001b[39m.format(new_fqn))\n",
      "\u001b[31mPy4JError\u001b[39m: py4j.reflection does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "# configuraciones de seguridad para fechas antiguas\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "# 2. Rutas\n",
    "silver_path = \"/app/data/lakehouse/silver/secop\"\n",
    "quarantine_path = \"/app/data/lakehouse/quarantine/secop_errors\"\n",
    "\n",
    "# 3. Guardado de SILVER \n",
    "print(\" Iniciando guardado de Silver...\")\n",
    "(df_silver\n",
    " .repartition(4) \n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"maxRecordsPerFile\", 50000) # Evita archivos gigantes que consumen mucha RAM\n",
    " .save(silver_path))\n",
    "\n",
    "# 4. Guardado de CUARENTENA\n",
    "print(\" Iniciando guardado de Cuarentena...\")\n",
    "(df_quarantine\n",
    " .repartition(2) \n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(quarantine_path))\n",
    "\n",
    "print(f\" Ingesta Silver exitosa\")\n",
    "print(f\" Registros en Cuarentena guardados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
